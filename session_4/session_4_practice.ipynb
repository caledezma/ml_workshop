{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e96270",
   "metadata": {},
   "source": [
    "# Practical session 4\n",
    "\n",
    "Most datasets that you will work with are going to be quite challenging, and one of the biggest challenges will be to find a model that not only does good in training data, but that also does good in data that has not been used for training. Especially as your data grows, there are two phenomena that you need to be aware of:\n",
    "\n",
    "1. When your model has poor training performance and poor testing performance, we say that the model has **underfit** the training data.\n",
    "2. When your model has good training performance and poor testing performance, we say that the model has **overfit** the training data.\n",
    "\n",
    "The idea of over- and under-fitting has to do with _capacity_ (i.e. how large is the search space where you expect to find your model). If your model has too much capcity (i.e. it can use very complex functions), then it is very likely to _overfit_ your training data. If your model has low capacity (i.e. it can only use very simple functions), then it is likely to underfit the data.\n",
    "\n",
    "You can control the capacity of most models by playing with the number of parameters. However, this can result in very inefficient optimisation; causing you to repeatedly train your model to find the right number of parameters for it. In machine learning, this challenge is solved through **inductive biases** and/or **regularisation**.\n",
    "\n",
    "Regularisation is \"any technique that is used to specifically improve the performance in the test set, regardless of the performance in the training set\". Common regularisation techniques (e.g. weight decay and dropout) will penalise the model for choosing too complex models that are likely to result in overfitting. That is, you give the model high capacity, but you limit its ability to choose too complex models; the idea is that the model you land on will be \"just right\".\n",
    "\n",
    "Inductive biases are specific relations that are put in the _mathematical definition_ of the ML models to reduce the search space. For example Convolutional Neural Networks (CNN) are a sub-set of MLPs where we force each perceptron to only process a subset from the previous layers. When processing images, this enforces a bias of locality; pixels that are close-by are processed together. CNNs are often the only way to get good result in image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7368d0f",
   "metadata": {},
   "source": [
    "## Practical exercise 1: hand-written digit recognition\n",
    "\n",
    "In this exercise we are going to explore the idea of regularisation. For this exercise we are going to use the MNIST dataset. The features of this dataset are measurable qualities of a wine (acidity, sugar, ...) and the target is the quality of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "random_gen = np.random.default_rng()\n",
    "random_numbers = random_gen.integers(low=0, high=x_train.shape[0], size=9)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "for idx, example in enumerate(random_numbers):\n",
    "    plt.subplot(3, 3, idx+1)\n",
    "    plt.imshow(x_train[example, :, :], cmap='gray')\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1520b4a",
   "metadata": {},
   "source": [
    "For the first exercise we are going to stick with the same model that we were using before, the MLP. We will now introduce two forms of regularisation:\n",
    "1. Dropout: makes the network randomly drop some perceptron connections during training. Which are dropped are random in each epoch.\n",
    "2. L2-regularisation: puts a penalty on high-value coefficients, helping the model prefer less complex models that generalise better.\n",
    "\n",
    "The objective is, again, to try to get the test accuracy as high as possible using the same parameters as before. Hint: in order to get good performance from the start, I suggest that you think of the size of the input and how many perceptrons you need in order to process all that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_UNITS = 1\n",
    "NUM_LAYERS = 1\n",
    "NUM_EPOCHS = 5\n",
    "DROPOUT = 0.1 # Usually less than 0.5\n",
    "REG_COEFFICIENT = 0.001 # Usually less than 0.001\n",
    "\n",
    "inputs = tf.keras.Input(shape=x_train.shape[1:])\n",
    "output = tf.keras.layers.Flatten(input_shape=x_train.shape[1:])(inputs)\n",
    "for _ in range(NUM_LAYERS):\n",
    "    output = tf.keras.layers.Dense(\n",
    "        units=NUM_UNITS,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(REG_COEFFICIENT),\n",
    "    )(output)\n",
    "    output = tf.keras.layers.Dropout(DROPOUT)(output)\n",
    "output = tf.keras.layers.Dense(\n",
    "    units=y_test.shape[1],\n",
    "    activation='softmax'\n",
    ")(output)\n",
    "model = tf.keras.Model(inputs, output)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train, epochs=NUM_EPOCHS)\n",
    "test_metrics = model.evaluate(x_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(np.arange(NUM_EPOCHS), np.array(history.history['accuracy']) * 100)\n",
    "plt.title(\n",
    "    \"Model accuracy v. epochs of training\\n\"\n",
    "    f\"Train Acc={round(history.history['accuracy'][-1]*100, 4)}%, Test Acc={round(test_metrics[1]*100, 4)}%\"\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training accuracy (%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e3bb2",
   "metadata": {},
   "source": [
    "## Practical exercise 2: introducing convolutions as an inductive bias\n",
    "\n",
    "We will now introduce a different architecture: the convolutional neural network (CNN). CNNs introduce an inductive bias of locality by reducing the number of connections available to the MLP. Here are some parameters for you to play with to achieve high performance:\n",
    "1. Convolutional layers: like the MLP, the CNN can build complex models from simpler ones, more layers means that the CNN will have more capacity.\n",
    "2. Kernel size: this is where the inductive bias comes in; this parameter says how many pixels are used to inform the activation of the perceptron. A 2x2 processes 4 adjacent pixels simultaneously, 3x3 processes 9 adjacent pixels, and so on...\n",
    "\n",
    "We introduce an MLP at the end to do the classification anyway.\n",
    "\n",
    "How does the test accuracy compare to the previous one? What about the training times? What about model size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CONV_LAYERS = 1\n",
    "KERNEL_SIZE = (3, 3)\n",
    "NUM_MLP_LAYERS = 1\n",
    "NUM_MLP_UNITS = 128\n",
    "NUM_EPOCHS = 5\n",
    "DROPOUT = 0.1 # usually less than 0.5\n",
    "REG_COEFFICIENT = 0.001 # Usually less than 0.001\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_train_conv = np.expand_dims(x_train, -1)\n",
    "x_test_conv = np.expand_dims(x_test, -1)\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=x_train_conv.shape[1:])\n",
    "output = inputs\n",
    "for _ in range(NUM_CONV_LAYERS):\n",
    "    output = tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=KERNEL_SIZE,\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "    )(output)\n",
    "\n",
    "output = tf.keras.layers.Flatten(input_shape=x_train.shape[1:])(output)\n",
    "for _ in range(NUM_MLP_LAYERS):\n",
    "    output = tf.keras.layers.Dense(\n",
    "        units=NUM_MLP_UNITS,\n",
    "        activation='relu'\n",
    "    )(output)\n",
    "    output = tf.keras.layers.Dropout(DROPOUT)(output)\n",
    "\n",
    "output = tf.keras.layers.Dense(\n",
    "    units=y_test.shape[1],\n",
    "    activation='softmax'\n",
    ")(output)\n",
    "model = tf.keras.Model(inputs, output)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "history = model.fit(x_train_conv, y_train, epochs=NUM_EPOCHS)\n",
    "test_metrics = model.evaluate(x_test_conv, y_test)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(np.arange(NUM_EPOCHS), np.array(history.history['accuracy']) * 100)\n",
    "plt.title(\n",
    "    \"Model accuracy v. epochs of training\\n\"\n",
    "    f\"Train Acc={round(history.history['accuracy'][-1]*100, 4)}%, Test Acc={round(test_metrics[1]*100, 4)}%\"\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training accuracy (%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05784e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop-env",
   "language": "python",
   "name": "ml-workshop-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
