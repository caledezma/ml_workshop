{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec7ccadf",
   "metadata": {},
   "source": [
    "# Practical session 4\n",
    "\n",
    "Most datasets that you will work with are going to be quite challenging, and one of the biggest challenges will be to find a model that not only does good in training data, but that also does good in data that has not been used for training. Especially as your data grows, there are two phenomena that you need to be aware of:\n",
    "\n",
    "1. When your model has poor training performance and poor testing performance, we say that the model has **underfit** the training data.\n",
    "2. When your model has good training performance and poor testing performance, we say that the model has **overfit** the training data.\n",
    "\n",
    "The idea of over- and under-fitting has to do with _capacity_ (i.e. how large is the search space where you expect to find your model). If your model has too much capcity (i.e. it can use very complex functions), then it is very likely to _overfit_ your training data. If your model has low capacity (i.e. it can only use very simple functions), then it is likely to underfit the data.\n",
    "\n",
    "You can control the capacity of most models by playing with the number of parameters. However, this can result in very inefficient optimisation; causing you to repeatedly train your model to find the right number of parameters for it. In machine learning, this challenge is solved through **inductive biases** and/or **regularisation**.\n",
    "\n",
    "Regularisation is \"any technique that is used to specifically improve the performance in the test set, regardless of the performance in the training set\". Common regularisation techniques (e.g. weight decay and dropout) will penalise the model for choosing too complex models that are likely to result in overfitting. That is, you give the model high capacity, but you limit its ability to choose too complex models; the idea is that the model you land on will be \"just right\".\n",
    "\n",
    "Inductive biases are specific relations that are put in the _mathematical definition_ of the ML models to reduce the search space. For example Convolutional Neural Networks (CNN) are a sub-set of MLPs where we force each perceptron to only process a subset from the previous layers. When processing images, this enforces a bias of locality; pixels that are close-by are processed together. CNNs are often the only way to get good result in image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd97d89",
   "metadata": {},
   "source": [
    "## Practical exercise 1: let's drink some wine\n",
    "\n",
    "In this exercise we are going to explore the idea of regularisation. For this exercise we are going to use a [wine quality dataset](https://www.kaggle.com/datasets/rajyellow46/wine-quality). The features of this dataset are measurable qualities of a wine (acidity, sugar, ...) and the target is the quality of the wine.\n",
    "\n",
    "The question is, can you predict if a wine is going to be good based on the input features? Give it a try!\n",
    "\n",
    "Observe that the data is re-split every time that you run the training code, this is to verify that you didn't just get a lucky split. The objective here is to get similar performance 3 times in a row, your submission will be the average over the three re-trainings. Regularisation will help make your training more stable and less dependant on the input examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Input your parameters here -- #\n",
    "NUM_EPOCHS = 50\n",
    "DEPTH = 5\n",
    "WIDTH = 5\n",
    "ACTIVATION = \"relu\"\n",
    "REG_COEFFICIENT = 0.001\n",
    "DROPOUT = 0.3\n",
    "\n",
    "## -- DO NOT TOUCH -- #\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_wine_data(filepath, test_size):\n",
    "    \"\"\"Get and prepare the wine data for ML\"\"\"\n",
    "    dataset = pd.read_csv(filepath)\n",
    "    num_classes = dataset.quality.max() + 1\n",
    "    dataset=dataset.to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset[:,:-2], dataset[:,-2],\n",
    "        test_size=test_size,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train,\n",
    "        X_test,\n",
    "        tf.keras.utils.to_categorical(y_train, num_classes=num_classes),\n",
    "        tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "    )\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_wine_data('wine_data.csv', test_size=0.2)\n",
    "\n",
    "input_ = tf.keras.Input(shape=X_train.shape[1])\n",
    "output = input_\n",
    "for _ in range(DEPTH):\n",
    "    output = tf.keras.layers.Dense(\n",
    "        units=WIDTH,\n",
    "        activation=ACTIVATION,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(REG_COEFFICIENT),\n",
    "    )(output)\n",
    "    output = tf.keras.layers.BatchNormalization()(output)\n",
    "    output=tf.keras.layers.Dropout(DROPOUT)(output)\n",
    "output = tf.keras.layers.Dense(\n",
    "    units=y_test.shape[1],\n",
    "    activation=\"softmax\",\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(REG_COEFFICIENT),\n",
    ")(output)\n",
    "model = tf.keras.Model(input_, output)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "print(model.summary())\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=2,\n",
    ")\n",
    "test_metrics = model.evaluate(\n",
    "    x=X_test,\n",
    "    y=y_test,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(np.arange(NUM_EPOCHS), np.array(history.history['loss']) * 100)\n",
    "plt.title(\n",
    "    \"Model accuracy v. epochs of training\\n\"\n",
    "    f\"Train Acc={history.history['accuracy'][-1]}, Test Acc={test_metrics[1]}\"\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "## -- END OF DO NOT TOUCH -- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16064bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop-env",
   "language": "python",
   "name": "ml-workshop-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
