{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab520d0",
   "metadata": {},
   "source": [
    "# Practical session 2\n",
    "\n",
    "In this session we are going to explore the concept of gradients and how they can be used to more efficiently find a model that is a good fit for the data.\n",
    "\n",
    "A gradient represents the direction in which a variable is heading with respect to another. Imagine that you have two related quantities A and B. If A increases as B increases, then the _gradient of A with respect to B_ is going to be positive. If A decreases as B increases, then the _gradient of A with respect to B_ is negative. Gradients also have a magnitude, which indicates how fast A will increase (or decrease) as B increases.\n",
    "\n",
    "Gradients are very useful because we have mathematical expressions for them. Which means that we don't need to see the plot of A vs. B to know how A changes with respect to B. If the gradient of A with respect to B is positive at some point, then increasing B will increase A. In a point where the gradient of A with respect to B is negative, increasing B will reduce A.\n",
    "\n",
    "In the modelling world. Gradients are central to finding good models automatically and efficiently. Think about it! If you define an error function for your \"model family\", you can calculate the gradient of the error with respect to your parameters. The sign of the gradient tells you how to change your parameter to reduce the error and the magnitude of the gradient tells you how fast the error will change!\n",
    "\n",
    "The method of following the opposite direction of the gradient to update your parameters is called _gradient descent_. It underpins nearly all machine learning algorithms.\n",
    "\n",
    "Using gradient descent means that you are no longer limited to optimising only models that you can see in a plot and you can update every parameter in your model simultaneously and know that the new error will be smaller than the previous one.\n",
    "\n",
    "\n",
    "## Practical exercise 1: a simple correlation, with help\n",
    "\n",
    "In this exercise we will get familiarised with how gradient descent works. We will use the same weights and heights dataset as before. This time, however, we have two extra tools at our disposal.\n",
    "\n",
    "The first one is the mean absolute error that the model makes on the observed data. We will use the mean absolute error to measure 'how bad' is our model. MAE is calculated as:\n",
    "\n",
    "MAE = 1/N sum(y_pred - y_true)\n",
    "\n",
    "The second tool at our disposal will be the gradients of the MAE with respect to the slope and cutoff point.\n",
    "\n",
    "Running the cell below you can set a slope and a cutoff point. The cell will print your model and the data (same as before), but it will also print the MAE and the gradients of each of the parameters.\n",
    "\n",
    "Your mission is to find a good slope and cutoff point using the gradient information to update the parameters at each try. As you do trial and error try to think of:\n",
    "\n",
    "- What happens if you only update one parameter at a time?\n",
    "- How low can you get the MAE? Can you get the MAE to 0?\n",
    "- What happens to the gradients as your MAE decreases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reading, run only once\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"heights_weights.csv\")\n",
    "height = data[\"Height(Inches)\"].to_numpy()\n",
    "weight = data[\"Weight(Pounds)\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66969e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Your parameters here -- ##\n",
    "\n",
    "SLOPE = \n",
    "CUTOFF = \n",
    "\n",
    "\n",
    "## --- NO EDIT START --- ##\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_slope = tf.Variable(float(SLOPE))\n",
    "tf_cutoff = tf.Variable(float(CUTOFF))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y_vals = height * tf_slope + tf_cutoff\n",
    "    mae = tf.reduce_mean(tf.abs(y_vals - weight))\n",
    "\n",
    "grad = tape.gradient(mae, [tf_slope, tf_cutoff])\n",
    "dslope = grad[0].numpy()\n",
    "dcutoff = grad[1].numpy()\n",
    "\n",
    "print(f\"Your current mean absolute error is {mae}\")\n",
    "print(f\"The gradient for the slope is {dslope}\")\n",
    "print(f\"The gradient for the cutoff is {dcutoff}\")\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(height, weight, s=1, label=\"data\")\n",
    "plt.plot(height, y_vals, 'y', label=\"model\")\n",
    "plt.xlabel(\"Height [Inches]\")\n",
    "plt.ylabel(\"Weight [Pounds]\")\n",
    "plt.legend()\n",
    "\n",
    "## --- NO EDIT END --- ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d38edd",
   "metadata": {},
   "source": [
    "## Practical exercise 2: another intergalactic experience\n",
    "\n",
    "We will now repeat the second exercise from the previous session, but with a twist. This time we have changed planets (so the same parameter as before won't work) and we are throwing the ball, not just letting it fall. This means that the A_1 parameter will not be 0 anymore.\n",
    "\n",
    "Your mission is to find which planet we are in now (i.e. what is the gravity) and what is the initial velocity at which we are throwing the ball. You have at your disposal the same tools as before: a mean absolute error and the gradients of that error with respect to each of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reading, run only once\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"thrown_fall.csv\")\n",
    "time = data[\"time\"].to_numpy()\n",
    "distance_travelled = data[\"distance\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f82dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Your parameters here -- ##\n",
    "\n",
    "A_0 = \n",
    "A_1 = \n",
    "A_2 = \n",
    "\n",
    "\n",
    "## --- NO EDIT START --- ##\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "tf_a0 = tf.Variable(float(A_0))\n",
    "tf_a1 = tf.Variable(float(A_1))\n",
    "tf_a2 = tf.Variable(float(A_2))\n",
    "\n",
    "sorted_time = np.array(sorted(time))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y_vals = tf_a0 + tf_a1 * sorted_time + tf_a2 * sorted_time ** 2\n",
    "    mae = tf.reduce_mean(tf.abs(y_vals - distance_travelled))\n",
    "\n",
    "grads = tape.gradient(mae, [tf_a0, tf_a1, tf_a2])\n",
    "\n",
    "da0 = grads[0].numpy()\n",
    "da1 = grads[1].numpy()\n",
    "da2 = grads[2].numpy()\n",
    "\n",
    "print(f\"Your current mean absolute error is {mae}\")\n",
    "print(f\"The gradient for a_0 is {da0}\")\n",
    "print(f\"The gradient for a_1 is {da1}\")\n",
    "print(f\"The gradient for a_2 is {da2}\")\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(time, distance_travelled, s=1, label=\"data\")\n",
    "plt.plot(sorted_time, y_vals, 'y', label=\"model\")\n",
    "plt.xlabel(\"Time [seconds]\")\n",
    "plt.ylabel(\"Distance Travelled [meters]\")\n",
    "plt.legend()\n",
    "\n",
    "## --- NO EDIT END --- ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3fa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop-env",
   "language": "python",
   "name": "ml-workshop-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
